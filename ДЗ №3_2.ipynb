{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4302924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a88455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\sem07\\AppData\\Local\\Temp\\ipykernel_25972\\3601153270.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  with open(\"C:\\\\Users\\sem07\\Desktop\\Курс 3 Семестр 2\\Семантика\\ДЗ №3\\obuchalka.txt\", \"r\", encoding=\"utf-8\") as f:\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка текста\n",
    "with open(\"\\obuchalka.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1112925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание уникальных символов + специальные токены\n",
    "chars = ['', ''] + sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Маппинг символов в индексы и наоборот\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# Функции кодирования и декодирования\n",
    "encode = lambda s: [stoi.get(c, stoi['']) for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Кодируем текст (с добавлением специальных токенов)\n",
    "encoded_data = encode(\" \" + text + \" \")\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8a8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Параметры\n",
    "block_size = 128\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "ff_hidden = 256\n",
    "n_epochs = 80\n",
    "dropout_rate=0.1\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5346bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "dataloader = DataLoader(CharDataset(data, block_size), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41c1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Модель\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=att.device)) == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, embed_dim),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, n_heads, n_layers, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, n_heads, ff_hidden)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))[None, :, :]\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4faa0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7912\n",
      "Epoch 10, Loss: 2.0669\n",
      "Epoch 20, Loss: 1.0997\n",
      "Epoch 30, Loss: 0.5605\n",
      "Epoch 40, Loss: 0.3627\n",
      "Epoch 50, Loss: 0.2710\n",
      "Epoch 60, Loss: 0.2618\n",
      "Epoch 70, Loss: 0.2096\n",
      "Epoch 79, Loss: 0.2007\n",
      "\n",
      "=== Сгенерированный текст ===\n",
      "Геолог сказал -  и чудовищным недоразумением, которое следовало сейчас же, немедленно разрешить. Валерий не мог этого сделать! Он не мог так низко пасть  — обворовать её, слабую девушку, своего школьного товарища, и п\n"
     ]
    }
   ],
   "source": [
    "# 6. Обучение\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniGPT(vocab_size, embedding_dim, block_size, n_heads, n_layers, ff_hidden).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "def generate(model, start_text=\"Привет\", max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    context = torch.tensor(encode(start_text), dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context_condensed = context[:, -block_size:]\n",
    "        logits = model(context_condensed)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        if top_k is not None:\n",
    "            values, indices = torch.topk(probs, top_k)\n",
    "            probs = torch.zeros_like(probs).scatter_(1, indices, values)\n",
    "            probs /= probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "\n",
    "    return decode(context[0].tolist())\n",
    "\n",
    "print(\"\\n=== Сгенерированный текст ===\")\n",
    "print(generate(model, start_text=\"Геолог сказал -  \", max_new_tokens=200, temperature=0.7, top_k=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
