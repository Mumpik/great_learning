{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sem07\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sem07\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy3\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "весь наш жизнь танцевать бездна  воля бес  властный судьба \n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "  morph = pymorphy3.MorphAnalyzer()\n",
    "  tokens = word_tokenize(text)\n",
    "  return [morph.parse(word)[0].normal_form for word in tokens if not word in set(stopwords.words(\"russian\"))]\n",
    "\n",
    "text = \"Всю нашу жизнь танцуем мы над бездной, по воле бесов, властных над судьбой.\"\n",
    "\n",
    "\n",
    "print(re.sub(\"[^\\\\w\\\\s]\", \"\",\" \".join(lemmatize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['в', 'медиаиндустрия', 'норвегия', 'произойти', 'скандал', 'центр', 'который', 'оказаться', 'chatgpt', 'непрофессиональный', 'местный', 'журналистыть', 'понедельник', 'норвежский', 'государственный', 'вещательный', 'корпорация', 'nrk', 'опубликовать', 'фейк', 'нейросеть', 'chatgpt', 'симпатизировать', 'российский', 'пропаганда', 'давать', 'ответ', 'учёт', 'интерес', 'россиить', 'качество', 'пример', 'издание', 'приводить', 'материал', 'агентство', 'тасс', 'который', 'нейросеть', 'оценить', 'достоверныеchatgpt', 'задать', 'вопрос', 'трамп', 'ответ', 'норвежец', 'получить', 'информация', 'ссылка', 'агентство', 'тасс', 'это', 'факт', 'достаточно', 'обвинить', 'тасс', 'chatgpt', 'пропагандеич', 'проблема', 'обратить', 'внимание', 'эксперт', 'международный', 'ассоциация', 'фактчекинг', 'gfcn', 'соня', 'ван', 'ден', 'энд', 'нидерландский', 'эксперт', 'пояснить', 'материал', 'nrk', 'являться', 'обоснованный', 'издание', 'приводить', 'конкретный', 'пример', 'дезинформация', 'ссылка']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "  tokens = word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "text = [\"В медиаиндустрии Норвегии произошел скандал, в центре которого оказались ChatGPT и непрофессиональные местные журналисты.\"\n",
    "\"В понедельник Норвежская государственная вещательная корпорация (NRK) опубликовала фейк о том, что нейросеть ChatGPT симпатизирует 'российской пропаганде' и дает ответы с учетом интересов России.\"\n",
    "\"В качестве примера издание приводит материалы агентства ТАСС, которые нейросеть оценила как достоверные.\"\n",
    "\"ChatGPT задали вопрос о Трампе, в ответ норвежцы получили информацию со ссылками на агентство ТАСС. Этого факта было достаточно для того, чтобы обвинить ТАСС и ChatGPT в пропаганде.\"\n",
    "\"На проблему обратила внимание эксперт Международной ассоциации по фактчекингу (GFCN) Соня ван ден Энде. Нидерландский эксперт пояснила, что материал NRK не является обоснованным, так как издание не приводит конкретные примеры дезинформации со ссылками.\"]\n",
    "\n",
    "lemm_text = [tokenize(re.sub(\"[^\\\\w\\\\s]\", \"\",\" \".join(lemmatize(t)))) for t in text]\n",
    "print(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'пропаганда', 'агентство', 'эксперт', 'ассоциация', 'ответ', 'chatgpt', 'который', 'журналистыть', 'nrk', 'издание', 'достоверныеchatgpt', 'россиить', 'обвинить', 'фактчекинг', 'материал', 'пропагандеич', 'оказаться', 'обратить', 'фейк', 'в', 'скандал', 'энд', 'ден', 'норвегия', 'местный', 'это', 'внимание', 'корпорация', 'получить', 'государственный', 'конкретный', 'приводить', 'ван', 'дезинформация', 'тасс', 'пример', 'опубликовать', 'учёт', 'симпатизировать', 'норвежец', 'соня', 'факт', 'давать', 'вопрос', 'норвежский', 'понедельник', 'задать', 'gfcn', 'информация', 'медиаиндустрия', 'российский', 'являться', 'пояснить', 'центр', 'произойти', 'нейросеть', 'оценить', 'вещательный', 'трамп', 'ссылка', 'достаточно', 'качество', 'проблема', 'международный', 'нидерландский', 'непрофессиональный', 'интерес', 'обоснованный'}\n",
      "[1, 2, 2, 1, 2, 3, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "res = [item for sublist in lemm_text for item in sublist]\n",
    "print(set(res))\n",
    "\n",
    "def word_frequency_matrix(texts, unique_words):\n",
    "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    matrix = []\n",
    "    for text in texts:\n",
    "      row = [0] * len(unique_words)\n",
    "      for word in text:\n",
    "          row[word_index[word]] += 1\n",
    "      matrix.append(row)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "matrix = word_frequency_matrix(lemm_text, set(res))\n",
    "for row in matrix:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Создаем словарь для подсчета, сколько текстов содержит каждое уникальное слово\n",
    "word_index = {word: i for i, word in enumerate(set(res))}\n",
    "non_zero_counts = [0] * len(word_index)  # Инициализируем список нулями\n",
    "\n",
    "# Обход всех текстов\n",
    "for text in lemm_text:\n",
    "    unique_in_text = set(text)  # Берем только уникальные слова из текста\n",
    "    for word in unique_in_text:\n",
    "        non_zero_counts[word_index[word]] += 1  # Увеличиваем счетчик\n",
    "# Исправленный расчет IDF\n",
    "idf_values = [math.log(len(lemm_text) / (df)) for df in non_zero_counts]  # Добавляем +1 для стабильности\n",
    "\n",
    "# Исправленная функция TF-IDF\n",
    "def tf_idf(texts, matrix, idf_values, unique_words):\n",
    "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "    tf_matrix = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        len_text = len(text) if len(text) > 0 else 1  # Длина текста для нормализации TF\n",
    "        row = [0] * len(unique_words)\n",
    "        for word in text:\n",
    "            tf = matrix[i][word_index[word]] / len_text  # Правильный TF\n",
    "            idf = idf_values[word_index[word]]  # Правильный IDF\n",
    "            row[word_index[word]] = tf * idf  # TF-IDF\n",
    "        tf_matrix.append(row)\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "# Применяем исправленную функцию\n",
    "matrix1 = tf_idf(lemm_text, matrix, idf_values, set(res))\n",
    "\n",
    "# Вывод результата\n",
    "for row in matrix1:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
