{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sem07\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sem07\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy3\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "весь наш жизнь танцевать бездна  воля бес  властный судьба \n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "  morph = pymorphy3.MorphAnalyzer()\n",
    "  tokens = word_tokenize(text)\n",
    "  return [morph.parse(word)[0].normal_form for word in tokens if not word in set(stopwords.words(\"russian\"))]\n",
    "\n",
    "text = \"Всю нашу жизнь танцуем мы над бездной, по воле бесов, властных над судьбой.\"\n",
    "\n",
    "\n",
    "print(re.sub(\"[^\\\\w\\\\s]\", \"\",\" \".join(lemmatize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['в', 'медиаиндустрия', 'норвегия', 'произойти', 'скандал', 'центр', 'который', 'оказаться', 'chatgpt', 'непрофессиональный', 'местный', 'журналистыть', 'понедельник', 'норвежский', 'государственный', 'вещательный', 'корпорация', 'nrk', 'опубликовать', 'фейк', 'нейросеть', 'chatgpt', 'симпатизировать', 'российский', 'пропаганда', 'давать', 'ответ', 'учёт', 'интерес', 'россиить', 'качество', 'пример', 'издание', 'приводить', 'материал', 'агентство', 'тасс', 'который', 'нейросеть', 'оценить', 'достоверныеchatgpt', 'задать', 'вопрос', 'трамп', 'ответ', 'норвежец', 'получить', 'информация', 'ссылка', 'агентство', 'тасс', 'это', 'факт', 'достаточно', 'обвинить', 'тасс', 'chatgpt', 'пропагандеич', 'проблема', 'обратить', 'внимание', 'эксперт', 'международный', 'ассоциация', 'фактчекинг', 'gfcn', 'соня', 'ван', 'ден', 'энд', 'нидерландский', 'эксперт', 'пояснить', 'материал', 'nrk', 'являться', 'обоснованный', 'издание', 'приводить', 'конкретный', 'пример', 'дезинформация', 'ссылка']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "  tokens = word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "text = [\"В медиаиндустрии Норвегии произошел скандал, в центре которого оказались ChatGPT и непрофессиональные местные журналисты.\"\n",
    "\"В понедельник Норвежская государственная вещательная корпорация (NRK) опубликовала фейк о том, что нейросеть ChatGPT симпатизирует 'российской пропаганде' и дает ответы с учетом интересов России.\"\n",
    "\"В качестве примера издание приводит материалы агентства ТАСС, которые нейросеть оценила как достоверные.\"\n",
    "\"ChatGPT задали вопрос о Трампе, в ответ норвежцы получили информацию со ссылками на агентство ТАСС. Этого факта было достаточно для того, чтобы обвинить ТАСС и ChatGPT в пропаганде.\"\n",
    "\"На проблему обратила внимание эксперт Международной ассоциации по фактчекингу (GFCN) Соня ван ден Энде. Нидерландский эксперт пояснила, что материал NRK не является обоснованным, так как издание не приводит конкретные примеры дезинформации со ссылками.\"]\n",
    "\n",
    "lemm_text = [tokenize(re.sub(\"[^\\\\w\\\\s]\", \"\",\" \".join(lemmatize(t)))) for t in text]\n",
    "print(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nrk', 'агентство', 'это', 'ден', 'непрофессиональный', 'в', 'издание', 'российский', 'достаточно', 'проблема', 'конкретный', 'центр', 'учёт', 'пример', 'норвежец', 'скандал', 'вопрос', 'обратить', 'ссылка', 'произойти', 'оказаться', 'пояснить', 'являться', 'достоверныеchatgpt', 'внимание', 'норвегия', 'который', 'оценить', 'трамп', 'нидерландский', 'фейк', 'фактчекинг', 'эксперт', 'задать', 'медиаиндустрия', 'chatgpt', 'корпорация', 'вещательный', 'энд', 'качество', 'симпатизировать', 'россиить', 'ван', 'местный', 'дезинформация', 'соня', 'получить', 'обоснованный', 'понедельник', 'материал', 'обвинить', 'журналистыть', 'тасс', 'факт', 'международный', 'интерес', 'gfcn', 'государственный', 'нейросеть', 'пропаганда', 'пропагандеич', 'опубликовать', 'ассоциация', 'норвежский', 'давать', 'информация', 'приводить', 'ответ'}\n",
      "[2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "#Уникальные слова\n",
    "res = [item for sublist in lemm_text for item in sublist]\n",
    "print(set(res))\n",
    "\n",
    "#Подсчет для каждого текста кол-во встреч. слов\n",
    "def word_frequency_matrix(texts, unique_words):\n",
    "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    matrix = []\n",
    "    for text in texts:\n",
    "      row = [0] * len(unique_words)\n",
    "      for word in text:\n",
    "          row[word_index[word]] += 1\n",
    "      matrix.append(row)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "matrix = word_frequency_matrix(lemm_text, set(res))\n",
    "for row in matrix:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.024096385542168676, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.03614457831325301, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.03614457831325301, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.012048192771084338, 0.024096385542168676, 0.024096385542168676]\n"
     ]
    }
   ],
   "source": [
    "#Транспонирование матрицы для удобного подсчета \"ненулей\" в строке(IDF)\n",
    "transposed = [list(col) for col in zip(*matrix)]\n",
    "non_zero_counts = [sum(1 for num in row if num != 0) for row in transposed]\n",
    "\n",
    "#TF-IDF\n",
    "def tf_idf(texts, matrix, non_zero, unique_words):\n",
    "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "    tf_matrix = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        len_text = len(text) if len(text) > 0 else 1\n",
    "        row = [0] * len(unique_words)\n",
    "        for word in text:\n",
    "            row[word_index[word]] = (matrix[i][word_index[word]] / len_text) * non_zero [word_index[word]]\n",
    "        tf_matrix.append(row)\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "matrix1 = tf_idf(lemm_text, matrix, non_zero_counts, set(res))\n",
    "for row in matrix1:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
